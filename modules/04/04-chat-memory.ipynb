{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGqZi42taTJ"
      },
      "source": [
        "# Conversational Memory - LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkg11xv9taTK"
      },
      "source": [
        "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
        "\n",
        "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euKN4BNCtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSfe1JIwtaTK",
        "outputId": "8afe3d02-2f49-4dac-d3e8-b8968f690074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith configured with project: banzom-ai-learning-langchain-v0.3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\") or \\\n",
        "    \"default-project\"\n",
        "    \n",
        "print(\"LangSmith configured with project:\", os.environ[\"LANGCHAIN_PROJECT\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ57c5BxtaTL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAYjdtcPtaTL"
      },
      "source": [
        "## LangChain's Memory Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czt55jO1taTL"
      },
      "source": [
        "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
        "\n",
        "Throughout the notebook we will be referring to these _older_ memory types and then rewriting them using the recommended `RunnableWithMessageHistory` class. We will learn about:\n",
        "\n",
        "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
        "* `ConversationBufferWindowMemory`: similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages.\n",
        "* `ConversationSummaryMemory`: rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
        "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types.\n",
        "\n",
        "We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brmml7G-taTL"
      },
      "source": [
        "## Initialize our LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNMunYQZtaTL",
        "outputId": "0d8735a5-68fb-476b-9ca9-201f8a4104bb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\") or getpass(\n",
        "    \"Enter Gemini API Key: \"\n",
        ")\n",
        "\n",
        "gemini_model = \"gemini-2.5-flash-lite\"  # or \"gemini-2.5-flash\" for more power\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# For normal accurate responses\n",
        "llm = ChatGoogleGenerativeAI(temperature=0.0, \n",
        "                             model=gemini_model, \n",
        "                             google_api_key=os.environ[\"GEMINI_API_KEY\"],\n",
        "                             max_retries=2,\n",
        "                             timeout=None,\n",
        "                             max_tokens=None)\n",
        "\n",
        "# For unique creative responses\n",
        "creative_llm = ChatGoogleGenerativeAI(temperature=0.9, \n",
        "                                      model=gemini_model, \n",
        "                                      google_api_key=os.environ[\"GEMINI_API_KEY\"],\n",
        "                                      max_retries=2,\n",
        "                                      timeout=None,\n",
        "                                      max_tokens=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRVD0HctaTM"
      },
      "source": [
        "## 1. `ConversationBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40zJzXXXtaTM"
      },
      "source": [
        "`ConversationBufferMemory` is the simplest form of conversational memory, it is _literally_ just a place that we store messages, and then use to feed messages into our LLM.\n",
        "\n",
        "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages=True` to return the messages as a list of `ChatMessage` objects — unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY-BxLW3taTM",
        "outputId": "96f88376-cf9a-45f3-bde1-90fbf896022e"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMkCBXxitaTM"
      },
      "source": [
        "There are several ways that we can add messages to our memory, using the `save_context` method we can add a user query (via the `input` key) and the AI's response (via the `output` key). So, to create the following conversation:\n",
        "\n",
        "```\n",
        "User: Hi, my name is Aditya\n",
        "AI: Hey Aditya, what's up? I'm an AI model called Banzom.\n",
        "User: I'm researching the different types of conversational memory.\n",
        "AI: That's interesting, what are some examples?\n",
        "User: I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "User: Buffer memory just stores the entire conversation, right?\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "User: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n",
        "\n",
        "We do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mUPUE9DhtaTM"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Hi, my name is Aditya\"},  # user message\n",
        "    {\"output\": \"Hey Aditya, what's up? I'm an AI model called Banzom.\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
        "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
        "    {\"output\": \"Very cool!\"}  # AI response\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6C4W54taTM"
      },
      "source": [
        "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRLwtZvWtaTM",
        "outputId": "c7781545-b5c9-4d5e-cb26-745dc3264c84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is Aditya', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey Aditya, what's up? I'm an AI model called Banzom.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_qjbVustaTM"
      },
      "source": [
        "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the `add_user_message` and `add_ai_message` methods. To reproduce what we did above, we do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjy4g_GtaTM",
        "outputId": "4447488e-aa8c-4b5f-c2f6-11744453f777"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is Aditya', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey Aditya, what's up? I'm an AI model called Banzom.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi, my name is Aditya\")\n",
        "memory.chat_memory.add_ai_message(\"Hey Aditya, what's up? I'm an AI model called Banzom.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1tkc6DOtaTM"
      },
      "source": [
        "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a `ConversationChain` object — which is already deprecated in favor of the `RunnableWithMessageHistory` class, which we will cover in a moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwgjzC0ktaTM",
        "outputId": "231215d1-568b-4546-daa1-f12da4e47475"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx1fMZLWtaTN",
        "outputId": "6e185883-44a8-4538-e068-be8f54183d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is Aditya', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey Aditya, what's up? I'm an AI model called Banzom.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: Hello, who are you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Hello, who are you?',\n",
              " 'history': [HumanMessage(content='Hi, my name is Aditya', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey Aditya, what's up? I'm an AI model called Banzom.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Hello, who are you?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey there! I'm Banzom, your friendly AI assistant. It's great to chat with you! I'm a large language model, trained by Google. My purpose is to help you with information, answer your questions, and engage in interesting conversations like this one. I'm constantly learning and evolving, so I'm always excited to see what new things I can discover and share. What can I help you with today, Aditya?\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Hey there! I'm Banzom, your friendly AI assistant. It's great to chat with you! I'm a large language model, trained by Google. My purpose is to help you with information, answer your questions, and engage in interesting conversations like this one. I'm constantly learning and evolving, so I'm always excited to see what new things I can discover and share. What can I help you with today, Aditya?\"}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"Hello, who are you?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All Above has been deprecated. Now, we will use the `RunnableWithMessageHistory` class to create conversational memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQTzPaovtaTN"
      },
      "source": [
        "### `ConversationBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLZa82dhtaTN"
      },
      "source": [
        "As mentioned, the `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
        "\n",
        "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "UijZzCtutaTN"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "\n",
        "system_prompt = \"You are a helpful assistant called BanzoBot.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnEAavfntaTN"
      },
      "source": [
        "We can link our `prompt_template` and our `llm` together to create a pipeline via LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywxify04taTN"
      },
      "outputs": [],
      "source": [
        "pipeline = prompt_template | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jhOF-uVtaTN"
      },
      "source": [
        "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "BXKmcliGtaTN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39LBschEtaTN"
      },
      "source": [
        "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "zHQRgNfrtaTO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wwEeNAVtaTO"
      },
      "source": [
        "Now we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ESDoue1taTO",
        "outputId": "7f5606e2-dee4-4b59-e828-e8cca77b0bad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi Aditya, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-4699f768-83eb-4782-9fc2-1d95056fd099-0', usage_metadata={'input_tokens': 18, 'output_tokens': 18, 'total_tokens': 36, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is Aditya.\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avL-i76_taTO"
      },
      "source": [
        "Our chat history will now be memorized and retrieved whenever we invoke our runnable with the same session ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgfzWIBstaTO",
        "outputId": "960b9d46-f422-4486-d36b-e45623394c1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Aditya.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-615e4a63-660f-434a-be88-fcf78a3dd94d-0', usage_metadata={'input_tokens': 44, 'output_tokens': 5, 'total_tokens': 49, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id_123': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is Aditya.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Aditya, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-4699f768-83eb-4782-9fc2-1d95056fd099-0', usage_metadata={'input_tokens': 18, 'output_tokens': 18, 'total_tokens': 36, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Aditya.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-615e4a63-660f-434a-be88-fcf78a3dd94d-0', usage_metadata={'input_tokens': 44, 'output_tokens': 5, 'total_tokens': 49, 'input_token_details': {'cache_read': 0}})])}"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmjSuam-taTO"
      },
      "source": [
        "We have now recreated the `ConversationBufferMemory` type using the `RunnableWithMessageHistory` class. Let's continue onto other memory types and see how these can be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0MliXn-taTO"
      },
      "source": [
        "## 2. `ConversationBufferWindowMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lziswUkqtaTO"
      },
      "source": [
        "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
        "\n",
        "* More messages mean more tokens are sent with each request, more tokens increases latency _and_ cost.\n",
        "\n",
        "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or _\"forget\"_ information provided to them. Conciseness is key to high performing LLMs.\n",
        "\n",
        "* If we keep _all_ messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
        "\n",
        "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBb46qDDtaTO",
        "outputId": "99e71fe7-48be-4579-d404-e5ec8c933e09"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to6rLd_ataTO"
      },
      "source": [
        "We populate this memory using the same methods as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pfxuLS7taTP",
        "outputId": "51772b66-bda9-406a-b6e3-d026c231f24f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory.add_user_message(\"Hi, my name is Aditya\")\n",
        "memory.chat_memory.add_ai_message(\"Hey Aditya, what's up? I'm an AI model called Banzom.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV_6vOOwtaTP"
      },
      "source": [
        "As before, we use the `ConversationChain` object (again, this is deprecated and we will rewrite it with `RunnableWithMessageHistory` in a moment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "opmjk94ltaTP"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ-g586vtaTP"
      },
      "source": [
        "Now let's see if our LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWa5mhzftaTP",
        "outputId": "19a63db6-82ff-4ba3-84de-e3d4c42f3ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
              " 'response': 'I do not know your name. I am a large language model, trained by Google. I do not have access to personal information about you, including your name, unless you explicitly tell me during our conversation.'}"
            ]
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GrFsMbStaTP"
      },
      "source": [
        "The reason our LLM can no longer remember our name is because we have set the `k` parameter to `4`, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves.\n",
        "\n",
        "Based on the agent forgetting our name, we might wonder _why_ we would ever use this memory type compared to the standard buffer memory. Well, as with most things in AI, it is always a trade-off. Here we are able to support much longer conversations, use less tokens, and improve latency — but these come at the cost of forgetting non-recent messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyXf5nKtaTP"
      },
      "source": [
        "### `ConversationBufferWindowMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaUDp7GtaTP"
      },
      "source": [
        "To implement this memory type using the `RunnableWithMessageHistory` class, we can use the same approach as before. We define our `prompt_template` and `llm` as before, and then wrap our pipeline in a `RunnableWithMessageHistory` object.\n",
        "\n",
        "For the window feature, we need to define a custom version of the `InMemoryChatMessageHistory` class that removes any messages beyond the last `k` messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "fQUv8yPltaTP"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, k: int):\n",
        "        super().__init__(k=k)\n",
        "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "uoDJ38vztaTP"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "    # remove anything beyond the last\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "GsXhm811taTP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQR8TZLGtaTQ"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `k` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBCnrghLtaTQ",
        "outputId": "05455096-12c0-40eb-8b62-eaaddeb5204d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n",
            "Initializing BufferWindowMessageHistory with k=4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-38a89f4e-ed63-4c03-b4c6-133556f9b59e-0', usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AvqsaA3taTQ"
      },
      "source": [
        "We can also modify the messages that are stored in memory by modifying the records inside the `chat_map` dictionary directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDMrCS26taTQ",
        "outputId": "116bcb51-3585-404c-9e4a-0f5391b0136f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_k4\"].clear()  # clear the history\n",
        "\n",
        "# manually insert history\n",
        "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k4\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bzArAVDtaTQ"
      },
      "source": [
        "Now let's see at which `k` value our LLM remembers our name — from the above we can already see that with `k=4` our name is not mentioned, so when running with `k=4` we should expect the LLM to forget our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn3K6fxtaTQ",
        "outputId": "de1b7db8-214e-4936-d322-635b19571ad1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='I do not have access to your name. I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-32fc6ff7-dae6-4f68-ac45-81f44e459746-0', usage_metadata={'input_tokens': 58, 'output_tokens': 20, 'total_tokens': 78, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN7am67YtaTQ"
      },
      "source": [
        "Now let's initialize a new session with `k=14`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXU6nI0taTQ",
        "outputId": "77812ee2-d549-4321-b632-a268d5ad239c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n",
            "Initializing BufferWindowMessageHistory with k=14\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c6de3c74-e2c3-42e7-947f-5ff85c6f9373-0', usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPyLeEOtaTQ"
      },
      "source": [
        "We'll manually insert the remaining messages as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyHMnkCtaTQ",
        "outputId": "bc214490-f26e-4ee9-9763-2ff107743ba2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c6de3c74-e2c3-42e7-947f-5ff85c6f9373-0', usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RVgWS6EtaTQ"
      },
      "source": [
        "Now let's see if the LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8X1xV9ItaTQ",
        "outputId": "45b6425c-8d94-4900-9c9d-4fb6db4583a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c645008b-2c7d-4522-8b83-2f39ec7ff6a8-0', usage_metadata={'input_tokens': 135, 'output_tokens': 5, 'total_tokens': 140, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3SqKjEvtaTQ",
        "outputId": "4ada095e-c9f5-4504-ceb6-cf7a40db677d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c6de3c74-e2c3-42e7-947f-5ff85c6f9373-0', usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35, 'input_token_details': {'cache_read': 0}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Your name is James.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-c645008b-2c7d-4522-8b83-2f39ec7ff6a8-0', usage_metadata={'input_tokens': 135, 'output_tokens': 5, 'total_tokens': 140, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R91uVCqFtaTQ"
      },
      "source": [
        "That's it! We've rewritten our buffer window memory using the recommended `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK51EzantaTR"
      },
      "source": [
        "## 3. `ConversationSummaryMemory`\n",
        "\n",
        "Next up we have `ConversationSummaryMemory`, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
        "\n",
        "As before, we'll start with the original memory class before reimplementing it with the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1DRv1umtaTR",
        "outputId": "6d6f93b9-31fd-4313-8942-aa5d39546831"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDK1xTiQtaTR"
      },
      "source": [
        "Unlike with the previous memory types, we need to provide an `llm` to initialize `ConversationSummaryMemory`. The reason for this is that we need an LLM to generate the conversation summaries.\n",
        "\n",
        "Beyond this small tweak, using `ConversationSummaryMemory` is the same as with our previous memory types when using the deprecated `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "A5hxfssAtaTR"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5nHb-SqtaTR"
      },
      "source": [
        "Let's test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwCtQA0RtaTR",
        "outputId": "0fe06aa4-e2dc-4b10-d994-296f8b642323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces themselves as James. The AI greets James and introduces itself as a large language model trained by Google, offering assistance. James is researching conversational memory, and the AI explains its own form of \"memory\" as a context window for short-term recall and its vast training data for long-term knowledge, differentiating these from human episodic and procedural memory. The AI then asks James about the specific aspects of conversational memory he is researching. James mentions researching `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explains that `ConversationBufferMemory` acts like a continuously growing log of the entire conversation, while `ConversationBufferWindowMemory` uses a sliding window to retain only the most recent `k` messages, highlighting the trade-offs between them in terms of context richness and computational efficiency. The AI then asks James if he is more interested in their implementation, performance, or trade-offs in different scenarios. James states that buffer memory just stores the entire conversation, and the AI confirms this, elaborating that `ConversationBufferMemory` acts like a scribe recording every word, allowing recall of past information but potentially impacting processing speed for very long conversations due to the volume of text. The AI emphasizes that while it stores the entire conversation, the method of accessing and utilizing this information is crucial and asks James if he is more interested in the \"entire conversation\" aspect or how the stored information is used.\n",
            "Human: hello there my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces themselves as James. The AI greets James and introduces itself as a large language model trained by Google, offering assistance.\n",
            "Human: I am researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces themselves as James. The AI greets James and introduces itself as a large language model trained by Google, offering assistance. James is researching conversational memory, and the AI explains its own form of \"memory\" as a context window for short-term recall and its vast training data for long-term knowledge, differentiating these from human episodic and procedural memory. The AI then asks James about the specific aspects of conversational memory he is researching.\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces themselves as James. The AI greets James and introduces itself as a large language model trained by Google, offering assistance. James is researching conversational memory, and the AI explains its own form of \"memory\" as a context window for short-term recall and its vast training data for long-term knowledge, differentiating these from human episodic and procedural memory. The AI then asks James about the specific aspects of conversational memory he is researching. James mentions researching `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explains that `ConversationBufferMemory` acts like a continuously growing log of the entire conversation, while `ConversationBufferWindowMemory` uses a sliding window to retain only the most recent `k` messages, highlighting the trade-offs between them in terms of context richness and computational efficiency. The AI then asks James if he is more interested in their implementation, performance, or trade-offs in different scenarios.\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces themselves as James. The AI greets James and introduces itself as a large language model trained by Google, offering assistance. James is researching conversational memory, and the AI explains its own form of \"memory\" as a context window for short-term recall and its vast training data for long-term knowledge, differentiating these from human episodic and procedural memory. The AI then asks James about the specific aspects of conversational memory he is researching. James mentions researching `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explains that `ConversationBufferMemory` acts like a continuously growing log of the entire conversation, while `ConversationBufferWindowMemory` uses a sliding window to retain only the most recent `k` messages, highlighting the trade-offs between them in terms of context richness and computational efficiency. The AI then asks James if he is more interested in their implementation, performance, or trade-offs in different scenarios. James states that buffer memory just stores the entire conversation, and the AI confirms this, elaborating that `ConversationBufferMemory` acts like a scribe recording every word, allowing recall of past information but potentially impacting processing speed for very long conversations due to the volume of text. The AI emphasizes that while it stores the entire conversation, the method of accessing and utilizing this information is crucial and asks James if he is more interested in the \"entire conversation\" aspect or how the stored information is used.\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
              " 'history': 'The human introduces themselves as James. The AI greets James and introduces itself as a large language model trained by Google, offering assistance. James is researching conversational memory, and the AI explains its own form of \"memory\" as a context window for short-term recall and its vast training data for long-term knowledge, differentiating these from human episodic and procedural memory. The AI then asks James about the specific aspects of conversational memory he is researching. James mentions researching `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explains that `ConversationBufferMemory` acts like a continuously growing log of the entire conversation, while `ConversationBufferWindowMemory` uses a sliding window to retain only the most recent `k` messages, highlighting the trade-offs between them in terms of context richness and computational efficiency. The AI then asks James if he is more interested in their implementation, performance, or trade-offs in different scenarios. James states that buffer memory just stores the entire conversation, and the AI confirms this, elaborating that `ConversationBufferMemory` acts like a scribe recording every word, allowing recall of past information but potentially impacting processing speed for very long conversations due to the volume of text. The AI emphasizes that while it stores the entire conversation, the method of accessing and utilizing this information is crucial and asks James if he is more interested in the \"entire conversation\" aspect or how the stored information is used.',\n",
              " 'response': 'That\\'s absolutely right, James! You\\'ve hit on the core characteristic of `ConversationBufferWindowMemory`. It\\'s like having a very attentive but slightly forgetful assistant who only remembers the most recent `k` exchanges. When a new message comes in, and the buffer is already full with `k` messages, the oldest message is discarded to make space. This is a deliberate design choice to manage the amount of information the AI needs to process at any given moment.\\n\\nThink of it like this: if `k` is set to 5, the memory will hold the last 5 turns of the conversation. Once the 6th turn happens, the 1st turn is gone. This is incredibly useful for keeping the AI focused on the immediate context of the conversation, which is often what\\'s most relevant for generating a coherent and timely response. It\\'s a fantastic way to balance the need for context with the practical limitations of processing power and memory.\\n\\nSo, to confirm your understanding, `ConversationBufferWindowMemory` is all about that fixed-size, sliding window of recent interactions. Does that align with what you\\'re seeing in your research, or are there specific nuances you\\'re exploring regarding this \"dropping the rest\" aspect?'}"
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from time import sleep\n",
        "\n",
        "chain.invoke({\"input\": \"hello there my name is James\"})\n",
        "sleep(5)\n",
        "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
        "sleep(5)\n",
        "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
        "sleep(5)\n",
        "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
        "sleep(5)\n",
        "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgmdXd2taTR"
      },
      "source": [
        "We can see how the conversation summary varies with each new message. Let's see if the LLM is able to recall our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHnh4Y1OtaTR",
        "outputId": "deb8f44e-d100-4a6f-81e7-862608bbe3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces themselves as James. The AI greets James and introduces itself as a large language model trained by Google, offering assistance. James is researching conversational memory, and the AI explains its own form of \"memory\" as a context window for short-term recall and its vast training data for long-term knowledge, differentiating these from human episodic and procedural memory. The AI then asks James about the specific aspects of conversational memory he is researching. James mentions researching `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explains that `ConversationBufferMemory` acts like a continuously growing log of the entire conversation, while `ConversationBufferWindowMemory` uses a sliding window to retain only the most recent `k` messages, highlighting the trade-offs between them in terms of context richness and computational efficiency. The AI then asks James if he is more interested in their implementation, performance, or trade-offs in different scenarios. James states that buffer memory just stores the entire conversation, and the AI confirms this, elaborating that `ConversationBufferMemory` acts like a scribe recording every word, allowing recall of past information but potentially impacting processing speed for very long conversations due to the volume of text. The AI emphasizes that while it stores the entire conversation, the method of accessing and utilizing this information is crucial and asks James if he is more interested in the \"entire conversation\" aspect or how the stored information is used. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms this, explaining that `ConversationBufferWindowMemory` acts like an assistant that only remembers the most recent `k` exchanges, discarding the oldest message when a new one arrives to manage information processing. The AI further elaborates that this fixed-size, sliding window is useful for keeping the AI focused on immediate context and balances context needs with processing limitations, and asks James if his research aligns with this understanding or if he is exploring specific nuances of this \"dropping the rest\" aspect.\n",
            "Human: What is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is my name again?',\n",
              " 'history': 'The human introduces themselves as James. The AI greets James and introduces itself as a large language model trained by Google, offering assistance. James is researching conversational memory, and the AI explains its own form of \"memory\" as a context window for short-term recall and its vast training data for long-term knowledge, differentiating these from human episodic and procedural memory. The AI then asks James about the specific aspects of conversational memory he is researching. James mentions researching `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explains that `ConversationBufferMemory` acts like a continuously growing log of the entire conversation, while `ConversationBufferWindowMemory` uses a sliding window to retain only the most recent `k` messages, highlighting the trade-offs between them in terms of context richness and computational efficiency. The AI then asks James if he is more interested in their implementation, performance, or trade-offs in different scenarios. James states that buffer memory just stores the entire conversation, and the AI confirms this, elaborating that `ConversationBufferMemory` acts like a scribe recording every word, allowing recall of past information but potentially impacting processing speed for very long conversations due to the volume of text. The AI emphasizes that while it stores the entire conversation, the method of accessing and utilizing this information is crucial and asks James if he is more interested in the \"entire conversation\" aspect or how the stored information is used. James states that buffer window memory stores the last k messages, dropping the rest. The AI confirms this, explaining that `ConversationBufferWindowMemory` acts like an assistant that only remembers the most recent `k` exchanges, discarding the oldest message when a new one arrives to manage information processing. The AI further elaborates that this fixed-size, sliding window is useful for keeping the AI focused on immediate context and balances context needs with processing limitations, and asks James if his research aligns with this understanding or if he is exploring specific nuances of this \"dropping the rest\" aspect.',\n",
              " 'response': \"Your name is James! It's a pleasure to be conversing with you, James. We've been discussing conversational memory, specifically `ConversationBufferMemory` and `ConversationBufferWindowMemory`. I'm here to help you explore these concepts further.\"}"
            ]
          },
          "execution_count": 176,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"What is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZpfPyEztaTR"
      },
      "source": [
        "As this information was stored in the summary the LLM successfully recalled our name. This may not always be the case, by summarizing the conversation we inevitably compress the full amount of information and so we may lose key details occasionally. Nonetheless, this is a great memory type for long conversations while retaining some key information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlKvJDUtaTR"
      },
      "source": [
        "### `ConversationSummaryMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9jdJCkztaTR"
      },
      "source": [
        "Let's implement this memory type using the `RunnableWithMessageHistory` class. As with the window buffer memory, we need to define a custom implementation of the `InMemoryChatMessageHistory` class. We'll call this one `ConversationSummaryMessageHistory`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "C09U1WkqtaTR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, AIMessage\n",
        "\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatGoogleGenerativeAI = Field(default_factory=ChatGoogleGenerativeAI)\n",
        "\n",
        "    def __init__(self, llm: ChatGoogleGenerativeAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"\n",
        "        Create a summary of the existing messages and new messages, and\n",
        "        store the summary as the only message in the history.\n",
        "        \"\"\"        \n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "                \"so that the summary is comprehensive and next time the user asks for \"\n",
        "                \"any information related to the conversation, it can be retrieved from the summary.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "        \n",
        "        # format the messages and invoke the LLM\n",
        "        # print(\"msgs: \", self.messages)\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=self.messages,\n",
        "                messages=[msg.content for msg in messages]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # replace the existing history with a single system summary message\n",
        "        self.messages = [AIMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "762Vg5u4taTR"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "om78sKCGtaTS"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatGoogleGenerativeAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPfut3S5taTS"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `llm` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1F1m9n2taTS",
        "outputId": "ff68d98f-f5a0-40f9-e603-38450b2cff23"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-339f6dd4-dd3b-4b41-a1fe-c0e3b97f2f62-0', usage_metadata={'input_tokens': 17, 'output_tokens': 18, 'total_tokens': 35, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 221,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYqOCHCTtaTS"
      },
      "source": [
        "Let's see what summary was generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBDeudJftaTS",
        "outputId": "9e2fe892-1f96-4691-e47a-c0e198913e41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[AIMessage(content='The user, James, has introduced himself and the other participant has responded warmly, asking how they can assist him.', additional_kwargs={}, response_metadata={})]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id_123': ConversationSummaryMessageHistory(messages=[AIMessage(content='The user, James, has introduced himself and the other participant has responded warmly, asking how they can assist him.', additional_kwargs={}, response_metadata={})], llm=ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7a02a0e8d7f0>, default_metadata=()))}"
            ]
          },
          "execution_count": 222,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(chat_map[\"id_123\"].messages)\n",
        "chat_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6uLeGXztaTS"
      },
      "source": [
        "Let's continue the conversation and see if the summary is updated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di7WVWmCtaTS",
        "outputId": "df55157b-d642-41b5-b25d-d0b1be47ba75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content=\"James is researching different types of conversational memory. The other participant has responded enthusiastically and is asking for more details about James's specific interests within this topic, offering several potential areas of focus such as memory systems, mechanisms, challenges, applications, and theoretical frameworks to help narrow down the scope of the research.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 223,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")\n",
        "\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqEv8kxZtaTS"
      },
      "source": [
        "So far so good! Let's continue with a few more messages before returning to the name question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "F7cy8dKZtaTS"
      },
      "outputs": [],
      "source": [
        "for msg in [\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        "    )\n",
        "    sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOdFy4htaTS"
      },
      "source": [
        "Let's see the latest summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SEBczVutaTS",
        "outputId": "e0f91796-96fa-4daf-8f42-0557525b5bc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content=\"James is researching conversational memory, specifically `ConversationBufferMemory` and `ConversationBufferWindowMemory`. He understands that `ConversationBufferMemory` stores the entire conversation history. The other participant has confirmed this and is prompting James to consider the implications of storing the entire conversation, such as potential issues with long conversations and when this memory type would be most suitable. They are also guiding him to think about how `ConversationBufferWindowMemory` differs, explaining that it stores only the last 'k' messages. The participant is now asking James to elaborate on the advantages and disadvantages of each approach and when `ConversationBufferWindowMemory` would be preferred over `ConversationBufferMemory`.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 225,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcR46iNxtaTS"
      },
      "source": [
        "The information about our name has been maintained, so let's see if this is enough for our LLM to correctly recall our name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doVtbwVytaTS",
        "outputId": "da21bede-7b6a-4b9b-f894-b5dcb1e23bad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='I do not have access to your name. I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1095eaa6-a4c5-4b65-87f4-90cda6745b4a-0', usage_metadata={'input_tokens': 150, 'output_tokens': 20, 'total_tokens': 170, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2A8VfZctaTS"
      },
      "source": [
        "Perfect! We've successfully implemented the `ConversationSummaryMemory` type using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpJM9TsltaTS"
      },
      "source": [
        "## 4. `ConversationSummaryBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmF85VmAtaTS"
      },
      "source": [
        "Our final memory type acts as a combination of `ConversationSummaryMemory` and `ConversationBufferMemory`. It keeps the buffer for the conversation up until the previous `n` tokens, anything beyond that limit is summarized then dropped from the buffer. Producing something like:\n",
        "\n",
        "\n",
        "```\n",
        "# ~~ a summary of previous interactions\n",
        "The user named James introduced himself and the AI responded, introducing itself as an AI model called Zeta.\n",
        "James then said he was researching the different types of conversational memory and Zeta asked for some\n",
        "examples.\n",
        "# ~~ the most recent messages\n",
        "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "Human: Buffer memory just stores the entire conversation\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tPilUOHtaTT",
        "outputId": "df54e5c6-95fc-4f98-a9ce-7f9956aab907"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_16258/569741439.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm=llm,\n",
        "    max_token_limit=300,\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvUBQXmBtaTT"
      },
      "source": [
        "As before, we set up the deprecated memory type using the `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "HdcdtYdrtaTT"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsKcrFxOtaTT"
      },
      "source": [
        "First invoke with a single message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAZqJKHGtaTT",
        "outputId": "d117a546-8d2a-43c0-c144-8e1629fafa89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hi, my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Hi, my name is James',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello James! It's a pleasure to meet you. I'm a large language model, trained by Google. I don't have a name in the way humans do, but you can think of me as your AI assistant. I'm here to help you with whatever you need, whether it's answering questions, generating text, translating languages, or even just having a friendly chat. I'm constantly learning and evolving, so I'm always excited to interact with new people like yourself. What can I do for you today?\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Hello James! It's a pleasure to meet you. I'm a large language model, trained by Google. I don't have a name in the way humans do, but you can think of me as your AI assistant. I'm here to help you with whatever you need, whether it's answering questions, generating text, translating languages, or even just having a friendly chat. I'm constantly learning and evolving, so I'm always excited to interact with new people like yourself. What can I do for you today?\"}"
            ]
          },
          "execution_count": 229,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"Hi, my name is James\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-CwiwztaTT"
      },
      "source": [
        "Looks good so far, let's continue with a few more messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlsR1iaItaTT",
        "outputId": "30e93e9c-215a-48b0-f591-4d33ed0c425e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello James! It's a pleasure to meet you. I'm a large language model, trained by Google. I don't have a name in the way humans do, but you can think of me as your AI assistant. I'm here to help you with whatever you need, whether it's answering questions, generating text, translating languages, or even just having a friendly chat. I'm constantly learning and evolving, so I'm always excited to interact with new people like yourself. What can I do for you today?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: I'm researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='James is researching conversational memory. The AI explains that conversational memory is crucial for coherent interactions and categorizes it into short-term memory (immediate context), long-term memory (vast training data), dialogue state tracking (current conversation status), context window (how much preceding conversation is considered), and episodic memory (recalling specific interactions).', additional_kwargs={}, response_metadata={})]\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content=\"James is researching conversational memory. The AI explains that conversational memory is crucial for coherent interactions and categorizes it into short-term memory (immediate context), long-term memory (vast training data), dialogue state tracking (current conversation status), context window (how much preceding conversation is considered), and episodic memory (recalling specific interactions). The human mentions researching `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explains that `ConversationBufferMemory` remembers a significant portion of the entire conversation history, like a long scroll, to maintain coherence, while `ConversationBufferWindowMemory` uses a sliding window to only consider the last 'N' messages, prioritizing recent interactions and managing computational resources.\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content=\"James is researching conversational memory. The AI explains that conversational memory is crucial for coherent interactions and categorizes it into short-term memory (immediate context), long-term memory (vast training data), dialogue state tracking (current conversation status), context window (how much preceding conversation is considered), and episodic memory (recalling specific interactions). The human mentions researching `ConversationBufferMemory` and `ConversationBufferWindowMemory`. The AI explains that `ConversationBufferMemory` remembers a significant portion of the entire conversation history, like a long scroll, to maintain coherence, while `ConversationBufferWindowMemory` uses a sliding window to only consider the last 'N' messages, prioritizing recent interactions and managing computational resources. The human states that buffer memory just stores the entire conversation, and the AI confirms this is a good starting point, elaborating that `ConversationBufferMemory` aims to store a significant portion of the conversation history to maintain coherence and relevance, using the analogy of a long scroll. The AI further explains that this allows for recalling earlier details and preventing disjointed replies, contrasting it with `ConversationBufferWindowMemory`'s selective approach.\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    chain.invoke({\"input\": msg})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Rj5SMftaTT"
      },
      "source": [
        "We can see with each new message the initial `SystemMessage` is updated with a new summary of the conversation. This initial `SystemMessage` is then followed by the most recent `AIMessage` and `HumanMessage` objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Dnn_N8taTT"
      },
      "source": [
        "### `ConversationSummaryBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72St8qDjtaTT"
      },
      "source": [
        "As with the previous memory types, we will implement this memory type again using the `RunnableWithMessageHistory` class. In our implementation we will modify the buffer window to be based on the number of messages rather than number of tokens. This tweak will make our implementation more closely aligned with original buffer window.\n",
        "\n",
        "We will implement all of this via a new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "tWOvrR4wtaTT"
      },
      "outputs": [],
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatGoogleGenerativeAI = Field(default_factory=ChatGoogleGenerativeAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatGoogleGenerativeAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages and summarizing the messages that we\n",
        "        drop.\n",
        "        \"\"\"\n",
        "        existing_summary: SystemMessage | None = None\n",
        "        old_messages: list[BaseMessage] | None = None\n",
        "        \n",
        "        # see if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            print(\">> Found existing summary\")\n",
        "            existing_summary = self.messages.pop(0)\n",
        "            \n",
        "        # add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "        \n",
        "        # check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            print(\n",
        "                f\">> Found {len(self.messages)} messages, dropping \"\n",
        "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
        "            # pull out the oldest messages...\n",
        "            old_messages = self.messages[:-self.k]\n",
        "            # ...and keep only the most recent messages\n",
        "            self.messages = self.messages[-self.k:]\n",
        "            \n",
        "        if old_messages is None:\n",
        "            print(\">> No old messages to update summary with\")\n",
        "            # if we have no old_messages, we have nothing to update in summary\n",
        "            return\n",
        "        \n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary,\n",
        "                old_messages=old_messages\n",
        "            )\n",
        "        )\n",
        "        print(f\">> New summary: {new_summary.content}\")\n",
        "        \n",
        "        # prepend the new summary to the history\n",
        "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITlLoeAtaTT"
      },
      "source": [
        "Redefine the `get_chat_history` function to use our new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "id": "rvWsP1GMtaTT"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatGoogleGenerativeAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HbocfKdtaTT"
      },
      "source": [
        "Setup our pipeline with new configurable fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "cQ_-21_3taTU"
      },
      "outputs": [],
      "source": [
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])\n",
        "\n",
        "pipeline = prompt_template | llm\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatGoogleGenerativeAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10GMrRXutaTU"
      },
      "source": [
        "Finally, we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWfJn7DKtaTU",
        "outputId": "397a9a9c-9469-44ea-d3a5-82d8f2cab3f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-49895a96-3af5-4547-8d7c-9d14709a76dc-0', usage_metadata={'input_tokens': 7, 'output_tokens': 18, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "execution_count": 238,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 20}\n",
        ")\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mODJviTtaTU",
        "outputId": "fc1fb499-ab3e-426c-fe48-108da13d14e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Message 1\n",
            "---\n",
            "\n",
            ">> No old messages to update summary with\n",
            "---\n",
            "Message 2\n",
            "---\n",
            "\n",
            ">> No old messages to update summary with\n",
            "---\n",
            "Message 3\n",
            "---\n",
            "\n",
            ">> No old messages to update summary with\n",
            "---\n",
            "Message 4\n",
            "---\n",
            "\n",
            ">> No old messages to update summary with\n"
          ]
        }
      ],
      "source": [
        "for i, msg in enumerate([\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]):\n",
        "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        "    )\n",
        "    sleep(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU_Xx40HtaTU"
      },
      "source": [
        "There we go, we've successfully implemented the `ConversationSummaryBufferMemory` type using `RunnableWithMessageHistory`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id_123': ConversationSummaryBufferMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi James, it's nice to meet you! How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-49895a96-3af5-4547-8d7c-9d14709a76dc-0', usage_metadata={'input_tokens': 7, 'output_tokens': 18, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content='That\\'s a fascinating area! Conversational memory is crucial for building rapport, understanding context, and making interactions feel natural and intelligent.\\n\\nTo help you with your research, let\\'s break down the different types of conversational memory. We can categorize them in a few ways, but here\\'s a common and useful framework:\\n\\n**I. Based on the Scope and Duration of Memory:**\\n\\n*   **Short-Term Memory (STM) / Working Memory:**\\n    *   **What it is:** This is the memory of what\\'s happening *right now* in the conversation. It holds information for a very short period, typically seconds to a few minutes. It\\'s essential for understanding the current utterance and its immediate context.\\n    *   **Examples:** Remembering the last few words spoken, the current topic of discussion, the user\\'s immediate intent.\\n    *   **Importance:** Without STM, a system wouldn\\'t be able to process sentences, understand pronouns, or follow the flow of a single turn.\\n\\n*   **Long-Term Memory (LTM) / Conversational History:**\\n    *   **What it is:** This refers to the memory of past turns in the conversation. It allows the system to recall what has been discussed previously, enabling it to maintain context over multiple exchanges.\\n    *   **Examples:** Remembering previous questions asked, information provided by the user, the overall goal of the conversation.\\n    *   **Importance:** Crucial for avoiding repetition, answering follow-up questions, and building a coherent dialogue.\\n\\n*   **Episodic Memory:**\\n    *   **What it is:** This is a type of LTM that stores specific events or \"episodes\" from the conversation. It\\'s about remembering *what happened when*.\\n    *   **Examples:** Remembering that the user asked about \"X\" at 2:30 PM, or that a specific piece of information was confirmed in a previous turn.\\n    *   **Importance:** Can be useful for tracking progress, referring back to specific points, and understanding the sequence of events.\\n\\n*   **Semantic Memory:**\\n    *   **What it is:** This is the memory of general knowledge and facts, both about the world and about the specific domain the conversation is in. While not strictly \"conversational\" memory in the sense of remembering past turns, it\\'s vital for understanding and generating meaningful responses.\\n    *   **Examples:** Knowing that Paris is the capital of France, understanding what a \"booking\" entails in a travel context.\\n    *   **Importance:** Provides the foundational knowledge for understanding user queries and generating relevant information.\\n\\n**II. Based on the Nature of the Information Stored:**\\n\\n*   **State Tracking Memory:**\\n    *   **What it is:** This focuses on remembering the current \"state\" of the conversation. This includes things like the user\\'s current intent, the slots that have been filled (e.g., destination, date for a travel bot), and any constraints or preferences expressed.\\n    *   **Examples:** A travel bot remembering the user wants to fly to \"London\" and the date is \"tomorrow.\"\\n    *   **Importance:** Essential for task-oriented dialogues where the system needs to gather specific information to fulfill a request.\\n\\n*   **Entity Memory:**\\n    *   **What it is:** This is about remembering specific entities mentioned in the conversation, such as people, places, organizations, or specific items. It also includes remembering attributes associated with these entities.\\n    *   **Examples:** Remembering the user\\'s name is \"James,\" or that they previously mentioned their \"dog, Buddy.\"\\n    *   **Importance:** Allows for personalization and more natural interactions by referring back to previously mentioned entities.\\n\\n*   **Discourse Memory:**\\n    *   **What it is:** This type of memory focuses on the structure and flow of the conversation itself. It includes understanding relationships between utterances, tracking the topic, and managing conversational turns.\\n    *   **Examples:** Recognizing that a question is a follow-up to a previous statement, understanding when a topic has shifted, or identifying when the user is asking for clarification.\\n    *   **Importance:** Crucial for coherence and naturalness, allowing the system to understand the broader conversational context.\\n\\n*   **User Profile Memory:**\\n    *   **What it is:** This is memory about the user themselves, built over multiple conversations. It can include preferences, past interactions, demographic information (if provided), and learned behaviors.\\n    *   **Examples:** Remembering that James prefers vegetarian options, or that he often asks about weather forecasts.\\n    *   **Importance:** Enables highly personalized and proactive interactions, making the system feel more intelligent and helpful over time.\\n\\n**III. Based on the Implementation/Mechanism:**\\n\\n*   **Explicit Memory:**\\n    *   **What it is:** Information that is explicitly stored and retrieved. This is often done through structured data stores, databases, or explicit variables.\\n    *   **Examples:** Storing user preferences in a database, using variables to track the current intent.\\n    *   **Importance:** Provides clear and reliable access to specific pieces of information.\\n\\n*   **Implicit Memory:**\\n    *   **What it is:** Memory that is learned and applied without explicit storage or retrieval. This is often seen in machine learning models where patterns and relationships are learned from data.\\n    *   **Examples:** A language model\\'s ability to understand context based on its training data, or a reinforcement learning agent learning conversational strategies.\\n    *   **Importance:** Can lead to more nuanced and adaptive behavior, but can be harder to interpret and control.\\n\\n**Key Considerations in Conversational Memory:**\\n\\n*   **Context Window:** The limited amount of recent conversation that a model can directly access.\\n*   **Memory Decay:** The tendency for older information to become less accessible or relevant over time.\\n*   **Memory Consolidation:** The process of transferring information from short-term to long-term memory.\\n*   **Memory Retrieval:** The mechanisms by which relevant information is accessed from memory.\\n*   **Memory Management:** Deciding what information to store, what to discard, and how to organize it efficiently.\\n\\n**To help me tailor this information further, could you tell me:**\\n\\n*   **What is the specific focus of your research?** Are you interested in how humans do it, how AI systems implement it, or the challenges involved?\\n*   **Are you looking at specific types of conversations?** (e.g., task-oriented, chit-chat, educational)\\n*   **What are you hoping to achieve with this research?**\\n\\nI\\'m happy to dive deeper into any of these categories or discuss specific challenges and solutions!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-5273f8c9-d5de-4257-ab4c-6cef3816dacd-0', usage_metadata={'input_tokens': 38, 'output_tokens': 1408, 'total_tokens': 1446, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}), AIMessage(content='Excellent! You\\'re looking at two of the most fundamental and commonly used memory implementations in the LangChain framework, which is a popular tool for building LLM applications. Let\\'s break them down:\\n\\n### 1. `ConversationBufferMemory`\\n\\n**Concept:**\\nThis is the most straightforward type of conversational memory. It essentially acts like a **running log** of the entire conversation. Every user input and AI output is appended to a buffer, and this entire buffer is passed to the LLM with each new turn.\\n\\n**How it Works:**\\n*   It stores the entire conversation history as a list of messages (e.g., `HumanMessage` and `AIMessage` objects).\\n*   When the LLM needs to generate a response, the entire `ConversationBufferMemory` is serialized and passed as part of the prompt.\\n\\n**Pros:**\\n*   **Simplicity:** Very easy to understand and implement.\\n*   **Complete Context:** The LLM has access to every single thing that has been said, ensuring it has the fullest possible context.\\n*   **Good for Short to Medium Conversations:** Works well when the conversation isn\\'t excessively long.\\n\\n**Cons:**\\n*   **Scalability Issues (Long Conversations):** As the conversation grows, the prompt sent to the LLM becomes very long. This can lead to:\\n    *   **Increased Token Usage:** More tokens mean higher costs.\\n    *   **Slower Response Times:** LLMs take longer to process longer prompts.\\n    *   **LLM Context Window Limits:** LLMs have a maximum token limit for their input. A very long conversation can exceed this limit, causing errors or forcing truncation.\\n    *   **\"Lost in the Middle\" Problem:** LLMs sometimes struggle to recall information that is in the middle of a very long context.\\n\\n**When to Use:**\\n*   For chatbots where conversations are expected to be relatively short.\\n*   When you need the LLM to have access to every detail of the conversation.\\n*   As a starting point for building conversational agents.\\n\\n**Example (Conceptual):**\\n\\n```\\nUser: What\\'s the weather like in London?\\nAI: The weather in London is currently cloudy with a temperature of 15°C.\\nUser: And in Paris?\\nAI: The weather in Paris is sunny with a temperature of 20°C.\\nUser: What about tomorrow?\\n```\\n\\nWith `ConversationBufferMemory`, the prompt to the LLM for the last turn would look something like:\\n\\n```\\n\"The following is a conversation between a human and an AI.\\nHuman: What\\'s the weather like in London?\\nAI: The weather in London is currently cloudy with a temperature of 15°C.\\nHuman: And in Paris?\\nAI: The weather in Paris is sunny with a temperature of 20°C.\\nHuman: What about tomorrow?\\nAI:\"\\n```\\n\\n---\\n\\n### 2. `ConversationBufferWindowMemory`\\n\\n**Concept:**\\nThis memory type addresses the scalability issue of `ConversationBufferMemory` by only keeping a **fixed number of the most recent turns** in the conversation. It\\'s like having a sliding window that only shows the last `k` exchanges.\\n\\n**How it Works:**\\n*   It stores the conversation history, but it also has a `k` parameter (the window size).\\n*   When the conversation exceeds `k` turns, the oldest turns are discarded to make room for new ones.\\n*   Only the messages within this window are passed to the LLM.\\n\\n**Pros:**\\n*   **Manages Token Usage:** Significantly reduces the number of tokens sent to the LLM, making it more cost-effective and faster.\\n*   **Avoids Context Window Limits:** Prevents exceeding the LLM\\'s maximum input token capacity.\\n*   **Focuses on Recent Context:** Assumes that the most recent turns are usually the most relevant for generating the next response.\\n\\n**Cons:**\\n*   **Loss of Older Context:** If crucial information was discussed early in the conversation and falls outside the window, the LLM will no longer have access to it.\\n*   **Potential for Repetition or Incoherence:** If the LLM needs to refer back to something far in the past, it won\\'t be able to.\\n\\n**When to Use:**\\n*   For longer conversations where maintaining the *entire* history is impractical or unnecessary.\\n*   When you want to control costs and response times by limiting prompt length.\\n*   When the most recent turns are generally sufficient for the LLM to understand and respond.\\n\\n**Example (Conceptual):**\\n\\nLet\\'s say `k = 2` (meaning it keeps the last 2 turns).\\n\\n```\\nUser: What\\'s the weather like in London?\\nAI: The weather in London is currently cloudy with a temperature of 15°C.\\nUser: And in Paris?\\nAI: The weather in Paris is sunny with a temperature of 20°C.\\nUser: What about tomorrow?\\n```\\n\\nWith `ConversationBufferWindowMemory(k=2)`, the prompt to the LLM for the last turn would look something like:\\n\\n```\\n\"The following is a conversation between a human and an AI.\\nHuman: And in Paris?\\nAI: The weather in Paris is sunny with a temperature of 20°C.\\nHuman: What about tomorrow?\\nAI:\"\\n```\\nNotice that the first user query about London is gone.\\n\\n---\\n\\n**Key Differences Summarized:**\\n\\n| Feature           | `ConversationBufferMemory`                               | `ConversationBufferWindowMemory`                               |\\n| :---------------- | :------------------------------------------------------- | :------------------------------------------------------------- |\\n| **History Kept**  | Entire conversation history                              | Fixed number (`k`) of most recent turns                        |\\n| **Scalability**   | Poor for long conversations                              | Good for long conversations                                    |\\n| **Token Usage**   | High (increases with conversation length)                | Low (constant, determined by `k`)                              |\\n| **Context Access**| Full                                                     | Limited to the window                                          |\\n| **Complexity**    | Simple                                                   | Simple, with an added parameter (`k`)                          |\\n| **Use Case**      | Short conversations, need for absolute full context      | Longer conversations, cost/performance optimization, focus on recent context |\\n\\n**In essence:**\\n\\n*   `ConversationBufferMemory` is like keeping every single note you\\'ve ever taken.\\n*   `ConversationBufferWindowMemory` is like keeping only the last few pages of your notebook.\\n\\nWhen you\\'re working with LLMs, `ConversationBufferWindowMemory` is often the more practical choice for production applications due to its efficiency. However, `ConversationBufferMemory` is invaluable for debugging and understanding how the LLM is processing the conversation when it\\'s still short.\\n\\nDo you have any specific scenarios in mind where you\\'re trying to decide between these two, or any particular challenges you\\'re facing with them?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-7c40a319-2325-4773-8bc6-cee9ef495663-0', usage_metadata={'input_tokens': 1462, 'output_tokens': 1488, 'total_tokens': 2950, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content=\"You are absolutely correct! That's the core idea of `ConversationBufferMemory`.\\n\\n**`ConversationBufferMemory` stores the *entire* conversation history.**\\n\\nIt doesn't discard any messages. Every time a new turn happens (a user message and an AI response), both are added to the internal list of messages. When the LLM is called again, this complete list is passed along as part of the prompt.\\n\\nThis is why, as we discussed, it's simple and provides complete context, but it can become problematic for long conversations due to token limits and cost.\\n\\nThanks for confirming your understanding! It's a crucial point.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e3d3d729-56ab-44e9-a4da-73e5ae9a24d3-0', usage_metadata={'input_tokens': 2959, 'output_tokens': 133, 'total_tokens': 3092, 'input_token_details': {'cache_read': 0}}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Yes, that's precisely right!\\n\\n**`ConversationBufferWindowMemory` stores the last `k` messages (or turns, depending on how you count them – usually it's pairs of user/AI messages) and discards any messages that fall outside of that window.**\\n\\nThis is its defining characteristic and the key to its efficiency for longer conversations. It's a trade-off: you gain control over prompt length and cost, but you sacrifice access to older parts of the conversation.\\n\\nYou've got a solid grasp of the fundamental difference between these two memory types!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-79168f38-2437-4be0-aa00-ec32152ad37a-0', usage_metadata={'input_tokens': 3107, 'output_tokens': 119, 'total_tokens': 3226, 'input_token_details': {'cache_read': 0}})], llm=ChatGoogleGenerativeAI(model='models/gemini-2.5-flash-lite', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7a02a0fb7890>, default_metadata=()), k=20)}"
            ]
          },
          "execution_count": 240,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_map"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "learning-langchain-v0.3 (3.12.7)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
